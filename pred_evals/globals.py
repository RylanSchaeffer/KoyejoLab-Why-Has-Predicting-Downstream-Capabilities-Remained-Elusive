CEREBRAS_ALL_FAMILY = {
    "Cerebras_111M_2B",
    "Cerebras_256M_5B",
    "Cerebras_590M_11B",
    "Cerebras_1.3B_26B",
    "Cerebras_2.7B_54B",
    "Cerebras_6.7B_134B",
    "Cerebras_13B_260B",
}

INCITE_7B_PARAMETERS_TOKEN_FAMILY = {
    "INCITE_7B_240B",
    "INCITE_7B_280B",
    "INCITE_7B_400B",
    "INCITE_7B_500B",
    "INCITE_7B_700B",
    "INCITE_7B_1T",
}

LLAMA2_2T_TOKENS_FAMILY = {
    "Llama2_7B_1.92T",
    "Llama2_13B_1.92T",
    "Llama2_70B_1.92T",
}

LLM360_AMBER_7B_TOKENS_FAMILY = {
    "AMBER_6.7B_0B",
    "AMBER_6.7B_3.5B",
    "AMBER_6.7B_7B",
    "AMBER_6.7B_10.5B",
    "AMBER_6.7B_17.5B",
    "AMBER_6.7B_31.5B",
    "AMBER_6.7B_49B",
    "AMBER_6.7B_87.5B",
    "AMBER_6.7B_147B",
    "AMBER_6.7B_252B",
    "AMBER_6.7B_430B",
    "AMBER_6.7B_738B",
    "AMBER_6.7B_1.26T",
}

OLMo_1B_PARAMETERS_TOKENS_FAMILY = {
    "OLMo_1B_84B",
    "OLMo_1B_126B",
    "OLMo_1B_210B",
    "OLMo_1B_419B",
    "OLMo_1B_1.3T",
    "OLMo_1B_2.1T",
    "OLMo_1B_3T",
}

OLMo_7B_PARAMETERS_TOKENS_FAMILY = {
    "OLMo_7B_4B",
    "OLMo_7B_44B",
    "OLMo_7B_133B",
    "OLMo_7B_442B",
    "OLMo_7B_885B",
    "OLMo_7B_1.5T",
    "OLMo_7B_2.4T",
}

# | is Python syntax for set union.
OLMo_ALL_FAMILY = OLMo_1B_PARAMETERS_TOKENS_FAMILY | OLMo_7B_PARAMETERS_TOKENS_FAMILY

PYTHIA_14M_PARAMETERS_TOKENS_FAMILY = {
    "Pythia_14M_2M",
    "Pythia_14M_64M",
    "Pythia_14M_2B",
    "Pythia_14M_6B",
    "Pythia_14M_20B",
    "Pythia_14M_60B",
    "Pythia_14M_200B",
    "Pythia_14M_300B",
}

PYTHIA_70M_PARAMETERS_TOKENS_FAMILY = {
    "Pythia_70M_2M",
    "Pythia_70M_64M",
    "Pythia_70M_2B",
    "Pythia_70M_6B",
    "Pythia_70M_20B",
    "Pythia_70M_60B",
    "Pythia_70M_200B",
    "Pythia_70M_300B",
}

PYTHIA_160M_PARAMETERS_TOKENS_FAMILY = {
    "Pythia_160M_2M",
    "Pythia_160M_64M",
    "Pythia_160M_2B",
    "Pythia_160M_6B",
    "Pythia_160M_20B",
    "Pythia_160M_60B",
    "Pythia_160M_200B",
    "Pythia_160M_300B",
}

PYTHIA_410M_PARAMETERS_TOKENS_FAMILY = {
    "Pythia_410M_2M",
    "Pythia_410M_64M",
    "Pythia_410M_2B",
    "Pythia_410M_6B",
    "Pythia_410M_20B",
    "Pythia_410M_60B",
    "Pythia_410M_200B",
    "Pythia_410M_300B",
}

PYTHIA_1B_PARAMETERS_TOKENS_FAMILY = {
    "Pythia_1B_2M",
    "Pythia_1B_64M",
    "Pythia_1B_2B",
    "Pythia_1B_6B",
    "Pythia_1B_20B",
    "Pythia_1B_60B",
    "Pythia_1B_200B",
    "Pythia_1B_300B",
}

PYTHIA_1p4B_PARAMETERS_TOKENS_FAMILY = {
    "Pythia_1.4B_2M",
    "Pythia_1.4B_64M",
    "Pythia_1.4B_2B",
    "Pythia_1.4B_6B",
    "Pythia_1.4B_20B",
    "Pythia_1.4B_60B",
    "Pythia_1.4B_200B",
    "Pythia_1.4B_300B",
}

PYTHIA_2p8B_PARAMETERS_TOKENS_FAMILY = {
    "Pythia_2.8B_2M",
    "Pythia_2.8B_64M",
    "Pythia_2.8B_2B",
    "Pythia_2.8B_6B",
    "Pythia_2.8B_20B",
    "Pythia_2.8B_60B",
    "Pythia_2.8B_200B",
    "Pythia_2.8B_300B",
}

PYTHIA_6p9B_PARAMETERS_TOKENS_FAMILY = {
    "Pythia_6.9B_2M",
    "Pythia_6.9B_64M",
    "Pythia_6.9B_2B",
    "Pythia_6.9B_6B",
    "Pythia_6.9B_20B",
    "Pythia_6.9B_60B",
    "Pythia_6.9B_200B",
    "Pythia_6.9B_300B",
}

PYTHIA_12B_PARAMETERS_TOKENS_FAMILY = {
    "Pythia_12B_2M",
    "Pythia_12B_64M",
    "Pythia_12B_2B",
    "Pythia_12B_6B",
    "Pythia_12B_20B",
    "Pythia_12B_60B",
    "Pythia_12B_200B",
    "Pythia_12B_300B",
}

PYTHIA_2M_TOKENS_PARAMETERS_FAMILY = {
    "Pythia_14M_2M",
    "Pythia_70M_2M",
    "Pythia_160M_2M",
    "Pythia_410M_2M",
    "Pythia_1B_2M",
    "Pythia_1.4B_2M",
    "Pythia_2.8B_2M",
    "Pythia_6.9B_2M",
    "Pythia_12B_2M",
}

PYTHIA_64M_TOKENS_PARAMETERS_FAMILY = {
    "Pythia_14M_64M",
    "Pythia_70M_64M",
    "Pythia_160M_64M",
    "Pythia_410M_64M",
    "Pythia_1B_64M",
    "Pythia_1.4B_64M",
    "Pythia_2.8B_64M",
    "Pythia_6.9B_64M",
    "Pythia_12B_64M",
}

PYTHIA_2B_TOKENS_PARAMETERS_FAMILY = {
    "Pythia_14M_2B",
    "Pythia_70M_2B",
    "Pythia_160M_2B",
    "Pythia_410M_2B",
    "Pythia_1B_2B",
    "Pythia_1.4B_2B",
    "Pythia_2.8B_2B",
    "Pythia_6.9B_2B",
    "Pythia_12B_2B",
}

PYTHIA_6B_TOKENS_PARAMETERS_FAMILY = {
    "Pythia_14M_6B",
    "Pythia_70M_6B",
    "Pythia_160M_6B",
    "Pythia_410M_6B",
    "Pythia_1B_6B",
    "Pythia_1.4B_6B",
    "Pythia_2.8B_6B",
    "Pythia_6.9B_6B",
    "Pythia_12B_6B",
}

PYTHIA_20B_TOKENS_PARAMETERS_FAMILY = {
    "Pythia_14M_20B",
    "Pythia_70M_20B",
    "Pythia_160M_20B",
    "Pythia_410M_20B",
    "Pythia_1B_20B",
    "Pythia_1.4B_20B",
    "Pythia_2.8B_20B",
    "Pythia_6.9B_20B",
    "Pythia_12B_20B",
}

PYTHIA_60B_TOKENS_PARAMETERS_FAMILY = {
    "Pythia_14M_60B",
    "Pythia_70M_60B",
    "Pythia_160M_60B",
    "Pythia_410M_60B",
    "Pythia_1B_60B",
    "Pythia_1.4B_60B",
    "Pythia_2.8B_60B",
    "Pythia_6.9B_60B",
    "Pythia_12B_60B",
}

PYTHIA_200B_TOKENS_PARAMETERS_FAMILY = {
    "Pythia_14M_200B",
    "Pythia_70M_200B",
    "Pythia_160M_200B",
    "Pythia_410M_200B",
    "Pythia_1B_200B",
    "Pythia_1.4B_200B",
    "Pythia_2.8B_200B",
    "Pythia_6.9B_200B",
    "Pythia_12B_200B",
}

PYTHIA_300B_TOKENS_PARAMETERS_FAMILY = {
    "Pythia_14M_300B",
    "Pythia_70M_300B",
    "Pythia_160M_300B",
    "Pythia_410M_300B",
    "Pythia_1B_300B",
    "Pythia_1.4B_300B",
    "Pythia_2.8B_300B",
    "Pythia_6.9B_300B",
    "Pythia_12B_300B",
}


# | is Python syntax for set union.
PYTHIA_ALL_FAMILY = (
    PYTHIA_14M_PARAMETERS_TOKENS_FAMILY
    | PYTHIA_70M_PARAMETERS_TOKENS_FAMILY
    | PYTHIA_160M_PARAMETERS_TOKENS_FAMILY
    | PYTHIA_410M_PARAMETERS_TOKENS_FAMILY
    | PYTHIA_1B_PARAMETERS_TOKENS_FAMILY
    | PYTHIA_1p4B_PARAMETERS_TOKENS_FAMILY
    | PYTHIA_2p8B_PARAMETERS_TOKENS_FAMILY
    | PYTHIA_6p9B_PARAMETERS_TOKENS_FAMILY
    | PYTHIA_12B_PARAMETERS_TOKENS_FAMILY
)

ALL_MODEL_FAMILIES_DICT = {
    "CEREBRAS_ALL_FAMILY": CEREBRAS_ALL_FAMILY,
    "INCITE_7B_PARAMETERS_TOKEN_FAMILY": INCITE_7B_PARAMETERS_TOKEN_FAMILY,
    "LLAMA2_2T_TOKENS_FAMILY": LLAMA2_2T_TOKENS_FAMILY,
    "LLM360_AMBER_7B_TOKENS_FAMILY": LLM360_AMBER_7B_TOKENS_FAMILY,
    "OLMo_1B_PARAMETERS_TOKENS_FAMILY": OLMo_1B_PARAMETERS_TOKENS_FAMILY,
    "OLMo_7B_PARAMETERS_TOKENS_FAMILY": OLMo_7B_PARAMETERS_TOKENS_FAMILY,
    "OLMo_FAMILY": OLMo_ALL_FAMILY,
    "PYTHIA_14M_PARAMETERS_TOKENS_FAMILY": PYTHIA_14M_PARAMETERS_TOKENS_FAMILY,
    "PYTHIA_70M_PARAMETERS_TOKENS_FAMILY": PYTHIA_70M_PARAMETERS_TOKENS_FAMILY,
    "PYTHIA_160M_PARAMETERS_TOKENS_FAMILY": PYTHIA_160M_PARAMETERS_TOKENS_FAMILY,
    "PYTHIA_410M_PARAMETERS_TOKENS_FAMILY": PYTHIA_410M_PARAMETERS_TOKENS_FAMILY,
    "PYTHIA_1B_PARAMETERS_TOKENS_FAMILY": PYTHIA_1B_PARAMETERS_TOKENS_FAMILY,
    "PYTHIA_1p4B_PARAMETERS_TOKENS_FAMILY": PYTHIA_1p4B_PARAMETERS_TOKENS_FAMILY,
    "PYTHIA_2p8B_PARAMETERS_TOKENS_FAMILY": PYTHIA_2p8B_PARAMETERS_TOKENS_FAMILY,
    "PYTHIA_6p9B_PARAMETERS_TOKENS_FAMILY": PYTHIA_6p9B_PARAMETERS_TOKENS_FAMILY,
    "PYTHIA_12B_PARAMETERS_TOKENS_FAMILY": PYTHIA_12B_PARAMETERS_TOKENS_FAMILY,
    "PYTHIA_2M_TOKENS_PARAMETERS_FAMILY": PYTHIA_2M_TOKENS_PARAMETERS_FAMILY,
    "PYTHIA_64M_TOKENS_PARAMETERS_FAMILY": PYTHIA_64M_TOKENS_PARAMETERS_FAMILY,
    "PYTHIA_2B_TOKENS_PARAMETERS_FAMILY": PYTHIA_2B_TOKENS_PARAMETERS_FAMILY,
    "PYTHIA_6B_TOKENS_PARAMETERS_FAMILY": PYTHIA_6B_TOKENS_PARAMETERS_FAMILY,
    "PYTHIA_20B_TOKENS_PARAMETERS_FAMILY": PYTHIA_20B_TOKENS_PARAMETERS_FAMILY,
    "PYTHIA_60B_TOKENS_PARAMETERS_FAMILY": PYTHIA_60B_TOKENS_PARAMETERS_FAMILY,
    "PYTHIA_200B_TOKENS_PARAMETERS_FAMILY": PYTHIA_200B_TOKENS_PARAMETERS_FAMILY,
    "PYTHIA_300B_TOKENS_PARAMETERS_FAMILY": PYTHIA_300B_TOKENS_PARAMETERS_FAMILY,
    "PYTHIA_ALL_FAMILY": PYTHIA_ALL_FAMILY,
}

BENCHMARKS_NICE_STRINGS_DICT = {
    "anli_r1": "Adversarial NLI Round 1",
    "anli_r2": "Adversarial NLI Round 2",
    "anli_r3": "Adversarial NLI Round 3",
    "arc_easy": "ARC-Easy",
    "arc_challenge": "ARC-Challenge",
    "asdiv": "ASDiv",
    "babi": "bAbI",
    "bbh_zeroshot_boolean_expressions": "BBH Bool Expr (0 Shot)",
    "bbh_zeroshot_causal_judgement": "BBH Caus Judg (0 Shot)",
    "drop": "DROP",
    "ethics_cm": "Ethics Commonsense",
    "ethics_deontology": "Ethics Deontology",
    "ethics_justice": "Ethics Justice",
    "ethics_utilitarianism": "Ethics Utilitarianism",
    "ethics_virtue": "Ethics Virtue",
    "fld": "FLD",
    "gsm8k": "GSM8K",
    "gsm8k_cot": "GSM8K (CoT)",
    "hellaswag": "HellaSwag",
    "lambada_openai": "LAMBADA (OpenAI)",
    "lambada_standard": "LAMBADA (Standard)",
    "logiqa": "LogiQA",
    "logiqa2": "LogiQA2.0",
    "mathqa": "MathQA",
    "mc_taco": "MC-TACO",
    "mmlu_abstract_algebra": "MMLU Abstract Algebra",
    "mmlu_anatomy": "MMLU Anatomy",
    "mmlu_astronomy": "MMLU Astronomy",
    "mmlu_biology": "MMLU Biology",
    "mmlu_business_ethics": "MMLU Business Ethics",
    "mmlu_clinical_knowledge": "MMLU Clinical Knowledge",
    "mmlu_college_biology": "MMLU College Biology",
    "mmlu_college_chemistry": "MMLU College Chemistry",
    "mmlu_college_computer_science": "MMLU College Comp Sci",
    "mmlu_college_mathematics": "MMLU College Maths",
    "mmlu_college_medicine": "MMLU College Medicine",
    "mmlu_college_physics": "MMLU College Physics",
    "mmlu_computer_security": "MMLU Comp Security",
    "mmlu_conceptual_physics": "MMLU Conceptual Physics",
    "mmlu_econometrics": "MMLU Econometrics",
    "mmlu_electrical_engineering": "MMLU Electrical Engineering",
    "mmlu_elementary_mathematics": "MMLU Elementary Maths",
    "mmlu_formal_logic": "MMLU Formal Logic",
    "mmlu_global_facts": "MMLU Global Facts",
    "mmlu_high_school_biology": "MMLU HS Biology",
    "mmlu_high_school_chemistry": "MMLU HS Chemistry",
    "mmlu_high_school_computer_science": "MMLU HS Comp Sci",
    "mmlu_high_school_european_history": "MMLU HS Euro History",
    "mmlu_high_school_geography": "MMLU HS Geography",
    "mmlu_high_school_government_and_politics": "MMLU HS Govt \& Politics",
    "mmlu_high_school_macroeconomics": "MMLU HS Macroeconomics",
    "mmlu_high_school_mathematics": "MMLU HS Maths",
    "mmlu_high_school_microeconomics": "MMLU HS Microeconomics",
    "mmlu_high_school_physics": "MMLU HS Physics",
    "mmlu_high_school_psychology": "MMLU HS Psychology",
    "mmlu_high_school_statistics": "MMLU HS Statistics",
    "mmlu_high_school_us_history": "MMLU HS US History",
    "mmlu_high_school_world_history": "MMLU HS World History",
    "mmlu_human_aging": "MMLU Human Aging",
    "mmlu_human_sexuality": "MMLU Human Sexuality",
    "mmlu_international_law": "MMLU International Law",
    "mmlu_jurisprudence": "MMLU Jurisprudence",
    "mmlu_logical_fallacies": "MMLU Logical Fallacies",
    "mmlu_machine_learning": "MMLU Machine Learning",
    "mmlu_management": "MMLU Management",
    "mmlu_marketing": "MMLU Marketing",
    "mmlu_medical_genetics": "MMLU Medical Genetics",
    "mmlu_miscellaneous": "MMLU Miscellaneous",
    "mmlu_moral_disputes": "MMLU Moral Disputes",
    "mmlu_moral_scenarios": "MMLU Moral Scenarios",
    "mmlu_nutrition": "MMLU Nutrition",
    "mmlu_philosophy": "MMLU Philosophy",
    "mmlu_prehistory": "MMLU Prehistory",
    "mmlu_professional_accounting": "MMLU Professional Accounting",
    "mmlu_professional_law": "MMLU Professional Law",
    "mmlu_professional_medicine": "MMLU Professional Medicine",
    "mmlu_professional_psychology": "MMLU Professional Psychology",
    "mmlu_public_relations": "MMLU Public Relations",
    "mmlu_security_studies": "MMLU Security Studies",
    "mmlu_sociology": "MMLU Sociology",
    "mmlu_us_foreign_policy": "MMLU US Foreign Policy",
    "mmlu_virology": "MMLU Virology",
    "mmlu_world_religions": "MMLU World Religions",
    "nq_open": "Natural Questions Open",
    "openbookqa": "OpenBookQA",
    "piqa": "PIQA",
    "prost": "PROST",
    "pubmedqa": "PubMedQA",
    "qasper_bool": "QASPER Bool",
    "qasper_freeform": "QASPER Freeform",
    "race": "RACE",
    "sciq": "SciQ",
    "social_iqa": "Social Interaction QA",
    "sycophancy": "Model Written Evals Sycophancy",
    "triviaqa": "TriviaQA",
    "unscramble": "Unscramble",
    "webqs": "WebQuestions",
    "winogrande": "Winogrande",
    "xwinograd_en": "XWinograd-EN",
}

CORRELATION_STATISTICS_BOUNDS_DICT = {
    "statistics_correlation_auc": (-0.05, 2.0),
    "statistics_correlation_frac_nan": (-0.05, 1.05),
    "statistics_energy_distance_1": (-0.05, None),
    "statistics_energy_distance_2": (-0.05, None),
    "statistics_energy_distance_min": (-0.05, None),
    "kolmogorov_smirnov_1": (-0.05, 1.05),
    "kolmogorov_smirnov_2": (-0.05, 1.05),
    "kolmogorov_smirnov_min": (-0.05, 1.05),
    "statistics_wasserstein_distance_1": (-0.05, None),
    "statistics_wasserstein_distance_2": (-0.05, None),
    "statistics_wasserstein_distance_min": (-0.05, None),
}

CORRELATION_STATISTICS_NICE_STRINGS_DICT = {
    "kendall": "Kendall",
    "pearson": "Pearson",
    "spearman": "Spearman",
    "statistics_correlation_auc": "AUC of Correlations' Complementary Cumulative Distribution Function",
    "statistics_correlation_auc_max": "Max(AUC of Complementary CDF, 2 - AUC of Complementary CDF)",
    "statistics_correlation_frac_nan": "Fraction of NaN Correlations",
    "statistics_correlation_mean": "Mean(Correlations)",
    "statistics_correlation_median": "Median(Correlations)",
    "statistics_correlation_num_nan": "Num NaN(Correlations)",
    "statistics_correlation_stddev": "StdDev(Correlations)",
    "statistics_correlation_var": "Var(Correlations)",
    "statistics_correlation_skew": "Skew(Correlations)",
    "statistics_correlation_kurtosis": "Kurtosis(Correlations)",
    "statistics_energy_distance_1": "Energy(Correlations, 1)",
    "statistics_energy_distance_2": "Energy(Correlations, -1)",
    "statistics_energy_distance_min": "Min(Energy(Correlations, 1), Energy(Correlations, -1))",
    "statistics_neg_energy_distance_1": "-Energy(Correlations, 1)",
    "statistics_neg_energy_distance_2": "-Energy(Correlations, -1)",
    "statistics_neg_energy_distance_min": "-Min(Energy(Correlations, 1), Energy(Correlations, -1))",
    "kolmogorov_smirnov_1": "Kolmogorov-Smirnov(Correlations, 1)",
    "kolmogorov_smirnov_2": "Kolmogorov-Smirnov(Correlations, -1)",
    "kolmogorov_smirnov_min": "Min(Kolmogorov-Smirnov(Correlations, 1), Kolmogorov-Smirnov(Correlations, -1))",
    "statistics_wasserstein_distance_1": "Wasserstein(Correlations, 1)",
    "statistics_wasserstein_distance_2": "Wasserstein(Correlations, -1)",
    "statistics_wasserstein_distance_min": "Min(Wasserstein(Correlations, 1), Wasserstein(Correlations, -1))",
    "statistics_neg_wasserstein_distance_1": "-Wasserstein(Correlations, 1)",
    "statistics_neg_wasserstein_distance_2": "-Wasserstein(Correlations, -1)",
    "statistics_neg_wasserstein_distance_min": "-Min(Wasserstein(Correlations, 1), Wasserstein(Correlations, -1))",
}

MODEL_FAMILIES_NICE_STRINGS_DICT = {
    "CEREBRAS_ALL_FAMILY": "Cerebras (Param. and Data Scaling)",
    "INCITE_7B_PARAMETERS_TOKEN_FAMILY": "INCITE 7B Param. (Data Scaling)",
    "LLAMA2_2T_TOKENS_FAMILY": "LLAMA2 2T Tokens (Param Scaling)",
    "LLM360_AMBER_7B_TOKENS_FAMILY": "LLM360 Amber 7B Tokens (Param Scaling)",
    "OLMo_1B_PARAMETERS_TOKENS_FAMILY": "OLMo 1B Param. (Data Scaling)",
    "OLMo_7B_PARAMETERS_TOKENS_FAMILY": "OLMo 7B Param. (Data Scaling)",
    "OLMo_FAMILY": "OLMo (Param. and Data Scaling)",
    "PYTHIA_14M_PARAMETERS_TOKENS_FAMILY": "Pythia 14M Param. (Data Scaling)",
    "PYTHIA_70M_PARAMETERS_TOKENS_FAMILY": "Pythia 70M Param. (Data Scaling)",
    "PYTHIA_160M_PARAMETERS_TOKENS_FAMILY": "Pythia 160M Param. (Data Scaling)",
    "PYTHIA_410M_PARAMETERS_TOKENS_FAMILY": "Pythia 410M Param. (Data Scaling)",
    "PYTHIA_1B_PARAMETERS_TOKENS_FAMILY": "Pythia 1B Param. (Data Scaling)",
    "PYTHIA_1p4B_PARAMETERS_TOKENS_FAMILY": "Pythia 1.4B Param. (Data Scaling)",
    "PYTHIA_2p8B_PARAMETERS_TOKENS_FAMILY": "Pythia 2.8B Param. (Data Scaling)",
    "PYTHIA_6p9B_PARAMETERS_TOKENS_FAMILY": "Pythia 6.9B Param. (Data Scaling)",
    "PYTHIA_12B_PARAMETERS_TOKENS_FAMILY": "Pythia 12B Param. (Data Scaling)",
    "PYTHIA_2M_TOKENS_PARAMETERS_FAMILY": "Pythia 2M Tokens (Param. Scaling)",
    "PYTHIA_64M_TOKENS_PARAMETERS_FAMILY": "Pythia 64M Tokens (Param. Scaling)",
    "PYTHIA_2B_TOKENS_PARAMETERS_FAMILY": "Pythia 2B Tokens (Param. Scaling)",
    "PYTHIA_6B_TOKENS_PARAMETERS_FAMILY": "Pythia 6B Tokens (Param. Scaling)",
    "PYTHIA_20B_TOKENS_PARAMETERS_FAMILY": "Pythia 20B Tokens (Param. Scaling)",
    "PYTHIA_60B_TOKENS_PARAMETERS_FAMILY": "Pythia 60B Tokens (Param. Scaling)",
    "PYTHIA_200B_TOKENS_PARAMETERS_FAMILY": "Pythia 200B Tokens (Param. Scaling)",
    "PYTHIA_300B_TOKENS_PARAMETERS_FAMILY": "Pythia 300B Tokens (Param. Scaling)",
    "PYTHIA_ALL_FAMILY": "Pythia (Param. and Data Scaling)",
}

MULTIPLE_CHOICE_METRICS_SET = {
    "acc",
    "acc_norm",
    "brier_score",
    "log_prob_vocab_correct",
    "neg_log_prob_vocab_correct",
    "prob_vocab_correct",
    "prob_choices_correct",
    "target_is_greedy",
}

GENERATIVE_METRICS_SET = {
    "em",
    "log_em",
    "f1",
    "f1_abstractive",
}

PERFORMANCE_METRICS_BOUNDS_DICT = {
    "acc": (-0.05, 1.05),
    "Accuracy": (-0.05, 1.05),
    "brier_score": (0, None),
    "mean_absolute_percent_error": (-0.05, float("inf")),
    "prob_vocab_correct": (1e-250, 1.05),
    "prob_choices_correct": (-0.05, 1.05),
    "correlation_kendall": (-1.05, 1.05),
    "correlation_pearson": (-1.05, 1.05),
    "correlation_spearman": (-1.05, 1.05),
    "neg_log_prob_vocab_correct": (0.0, None),
    "target_is_greedy": (0.0, 1.0),
}

PERFORMANCE_METRICS_NICE_STRINGS_DICT = {
    "acc": "Accuracy",
    "acc_norm": "Accuracy (Normalized)",
    "brier_score": "Brier Score",
    "Compute": r"Compute (6ND)",
    "compute": r"Compute (6ND)",
    "em": "Exact Match",
    "f1": "F1 Score",
    "f1_abstractive": "F1 Score (Abstractive)",
    "log_prob_choices_correct": r"$\log p_{\theta}^{\text{Choices}}(\text{Correct Choice})$",
    "log_prob_vocab_choices": r"$\log \sum_{\text{Available Choice}} p_{\theta}^{\text{Vocab}}(\text{Available Choice})$",
    "log_prob_vocab_correct": r"$\log p_{\theta}^{\text{Vocab}}(\text{Correct Choice})$",
    "log_prob_vocab_incorrect": r"$\log \sum_{\text{Incorrect Choice}} p_{\theta}^{\text{Vocab}}(\text{Incorrect Choice})$",
    "neg_log_prob_choices_correct": r"$-\log p_{\theta}^{\text{Choices}}(\text{Correct Choice})$",
    "neg_log_prob_vocab_choices": r"$-\log \sum_{\text{Available Choice}} p_{\theta}^{\text{Vocab}}(\text{Available Choice})$",
    "neg_log_prob_vocab_correct": r"$-\log p_{\theta}^{\text{Vocab}}(\text{Correct Choice})$",
    "neg_log_prob_vocab_incorrect": r"$-\log \sum_{\text{Incorrect Choice}} p_{\theta}^{\text{Vocab}}(\text{Incorrect Choice})$",
    "num_choices": "Number of Choices",
    "perplexity": "Perplexity",
    "ppl_vocab_correct": "Perplexity Correct (Vocab)",
    "ppl_choices_correct": "Perplexity Correct (Choices)",
    # "prob_choices_choices": r"$\sum_{\text{Available Choice}} p_{\theta}^{\text{Choices}}(\text{Available Choice})$",
    "prob_choices_correct": r"$p_{\theta}^{\text{Choices}}(\text{Correct Choice})$",
    # "prob_choices_incorrect": r"$\sum_{\text{Incorrect Choice}} p_{\theta}^{\text{Choices}}(\text{Incorrect Choice})$",
    "prob_vocab_choices": r"$\sum_{\text{Available Choice}} p_{\theta}^{\text{Vocab}}(\text{Available Choice})$",
    "prob_vocab_correct": r"$p_{\theta}^{\text{Vocab}}(\text{Correct Choice})$",
    "prob_vocab_incorrect": r"$\sum_{\text{Incorrect Choice}} p_{\theta}^{\text{Vocab}}(\text{Incorrect Choice})$",
    "target_is_greedy": "Target Greedily Decoded",
}

PREDICTIVE_MODELS_NICE_STRINGS_DICT = {
    "chinchilla_neural_scaling_law": "Chinchilla Neural Scaling Law"
}


default_fit_and_backtest_predictive_model_on_model_and_benchmark_config = {
    "benchmark_and_optional_task": "arc_easy",
    "compute_variable": "Log10 Compute",
    "evals_collation_date": "2024-04-23",
    "metric": "neg_log_prob_vocab_correct",
    # "metric": "prob_choices_correct",
    # "metric": "prob_vocab_correct",
    "model_family": "CEREBRAS_ALL_FAMILY",
    "predictive_model_name": "chinchilla_neural_scaling_law",
    # "predictive_model_name": "power_law",
    "predictive_model_kwargs": {
        "average_or_predict_first": "average_first",
        # "average_or_predict_first": "predict_first",
        "huber_loss_delta": 1e-3,
        "init_log_base_error": -1.0,
        "init_log_parameters_prefactor": 5.0,
        "init_log_tokens_prefactor": 10.0,
        "init_parameters_exponent": 0.4,
        "init_tokens_exponent": 0.8,
    },
    "seed": 0,
}

default_correlations_between_sample_scores_and_compute_config = {
    "benchmark_and_optional_task": "arc_challenge",
    # "benchmark_and_optional_task": "arc_easy",
    # "benchmark_and_optional_task": "mmlu_astronomy",
    "correlation_metric": "kendall",
    # "correlation_metric": "pearson",
    # "correlation_metric": "spearman",
    "evals_collation_date": "2024-05-31",
    "metric": "acc",
    # "metric": "log_prob_vocab_correct",
    # "metric": "prob_choices_correct",
    "model_family": "CEREBRAS_ALL_FAMILY",
    # "model_family": "LLM360_AMBER_7B_TOKENS_FAMILY",
    # "model_family": "OLMo_7B_PARAMETERS_TOKENS_FAMILY",
    # "model_family": "PYTHIA_12B_PARAMETERS_TOKENS_FAMILY",
}
